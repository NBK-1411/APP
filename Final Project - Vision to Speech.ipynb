{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5671dc-bb48-4598-8e55-b37f7faf5947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was'a man is standing next to a car'. There was'a man is walking across the street with a car'. There was'a horse race is being watched by a camera'. There was'a man is playing with a small dog'. There was'a man in a white shirt and black pants'. There was'a man in a white shirt and black pants is playing with a frc'. There was'a woman in a white dress is playing golf'. There was'a man in a white shirt and pants playing a game of cricket'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Initialize the semantic similarity model\n",
    "semantic_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def frame_to_text(frame):\n",
    "    \"\"\"Convert a single frame to text using the model.\"\"\"\n",
    "    image = Image.fromarray(frame)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=50)  # Specify max_new_tokens\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def get_unique_meanings(texts, threshold=0.8):\n",
    "    \"\"\"Filter out texts that have similar meanings based on semantic similarity.\"\"\"\n",
    "    unique_texts = []\n",
    "    for text in texts:\n",
    "        text_embedding = semantic_model.encode(text, convert_to_tensor=True)\n",
    "        is_unique = True\n",
    "        for unique_text in unique_texts:\n",
    "            unique_text_embedding = semantic_model.encode(unique_text, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(text_embedding, unique_text_embedding).item()\n",
    "            if similarity > threshold:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_texts.append(text)\n",
    "    return unique_texts\n",
    "\n",
    "def video_to_text(video_path, repeat_threshold=5):\n",
    "    \"\"\"Process video frames and generate meaningful text descriptions based on repeated actions.\"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    current_action = None\n",
    "    action_count = 0\n",
    "    frame_texts = []\n",
    "    \n",
    "    while True:\n",
    "        # Read each frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to text\n",
    "        text = frame_to_text(frame)\n",
    "        \n",
    "        if text == current_action:\n",
    "            action_count += 1\n",
    "        else:\n",
    "            if action_count >= repeat_threshold and current_action is not None:\n",
    "                frame_texts.append(f\"There was'{current_action}'.\")\n",
    "            current_action = text\n",
    "            action_count = 1\n",
    "    \n",
    "    # Append the last action if it meets the threshold\n",
    "    if action_count >= repeat_threshold and current_action is not None:\n",
    "        frame_texts.append(f\"The action '{current_action}'.\")\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Filter out similar meanings\n",
    "    unique_frame_texts = get_unique_meanings(frame_texts)\n",
    "    \n",
    "    # Combine all frame texts into meaningful sentences\n",
    "    video_description = \" \".join(unique_frame_texts)\n",
    "    return video_description\n",
    "\n",
    "# Example usage\n",
    "video_path = \"/Users/ronny/Downloads/AI_Final/testvideo.mp4\"\n",
    "description = video_to_text(video_path)\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1cee09-0196-4bc9-8443-c40c63102910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "def text_to_speech(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"output.mp3\")\n",
    "    os.system(\"afplay output.mp3\")  \n",
    "    \n",
    "text_to_speech(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d2d09d-e5a9-451d-ba63-17e75b7f952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Description: There was 'a man is standing next to a car'. There was 'a man is walking across the street with a car'. There was 'a horse race is being watched by a camera'. There was 'a man is playing with a small dog'. There was 'a man in a white shirt and black pants'. There was 'a man in a white shirt and black pants is playing with a frc'. There was 'a woman in a white dress is playing golf'. There was 'a man in a white shirt and pants playing a game of cricket'.\n",
      "Translated Description: Akwai 'wani mutum yana tsaye kusa da mota'.Akwai 'wani mutum yana tafiya a saman titi tare da mota'.An kalli 'tseren doki ta hanyar kamara'.Akwai 'wani mutum yana wasa da karamin kare'.Akwai 'wani mutum a cikin farin rigar da wando baki'.Akwai 'wani mutum a cikin farin riguna da wando baÆ™i suna wasa tare da FRC'.Akwai 'mace a cikin fararen fata tana wasa golf'.Akwai 'wani mutum a cikin farin rig rigar da wando suna wasa da wasan wasan kurket'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "\n",
    "# Disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Initialize the semantic similarity model\n",
    "semantic_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def frame_to_text(frame):\n",
    "    \"\"\"Convert a single frame to text using the model.\"\"\"\n",
    "    image = Image.fromarray(frame)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def get_unique_meanings(texts, threshold=0.8):\n",
    "    \"\"\"Filter out texts that have similar meanings based on semantic similarity.\"\"\"\n",
    "    unique_texts = []\n",
    "    for text in texts:\n",
    "        text_embedding = semantic_model.encode(text, convert_to_tensor=True)\n",
    "        is_unique = True\n",
    "        for unique_text in unique_texts:\n",
    "            unique_text_embedding = semantic_model.encode(unique_text, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(text_embedding, unique_text_embedding).item()\n",
    "            if similarity > threshold:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_texts.append(text)\n",
    "    return unique_texts\n",
    "\n",
    "def video_to_text(video_path, repeat_threshold=5):\n",
    "    \"\"\"Process video frames and generate meaningful text descriptions based on repeated actions.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    current_action = None\n",
    "    action_count = 0\n",
    "    frame_texts = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        text = frame_to_text(frame)\n",
    "\n",
    "        if text == current_action:\n",
    "            action_count += 1\n",
    "        else:\n",
    "            if action_count >= repeat_threshold and current_action is not None:\n",
    "                frame_texts.append(f\"There was '{current_action}'.\")\n",
    "            current_action = text\n",
    "            action_count = 1\n",
    "\n",
    "    if action_count >= repeat_threshold and current_action is not None:\n",
    "        frame_texts.append(f\"The action '{current_action}'.\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    unique_frame_texts = get_unique_meanings(frame_texts)\n",
    "    video_description = \" \".join(unique_frame_texts)\n",
    "    return video_description\n",
    "\n",
    "def translate_text(text, target_lang='ha'):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, dest=target_lang)\n",
    "    return translation.text\n",
    "\n",
    "def text_to_speech(text, lang='en'):\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(\"output.mp3\")\n",
    "    os.system(\"afplay output.mp3\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"/Users/ronny/Downloads/AI_Final/testvideo.mp4\"\n",
    "description = video_to_text(video_path)\n",
    "print(\"Original Description:\", description)\n",
    "\n",
    "# Translate the description to Twi\n",
    "translated_description = translate_text(description, target_lang='ha')\n",
    "print(\"Translated Description:\", translated_description)\n",
    "\n",
    "# Convert the translated text to speech\n",
    "text_to_speech(translated_description, lang='ha')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323544d-f2f4-4bdd-90de-9eb0d403a517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
