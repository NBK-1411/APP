{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9b4f71-982d-4684-bede-2f4dcaf5442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from gtts import gTTS\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa3086-bd1a-4af7-aa27-529366b7dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b65b66-5915-47fe-9768-3f0862949b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9e3d38-4f26-4436-a16d-95ae687d45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Initialize the semantic similarity model\n",
    "semantic_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "def frame_to_text(frame):\n",
    "    \"\"\"Convert a single frame to text using the model.\"\"\"\n",
    "    image = Image.fromarray(frame)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def get_unique_meanings(texts, threshold=0.8):\n",
    "    \"\"\"Filter out texts that have similar meanings based on semantic similarity.\"\"\"\n",
    "    unique_texts = []\n",
    "    for text in texts:\n",
    "        text_embedding = semantic_model.encode(text, convert_to_tensor=True)\n",
    "        is_unique = True\n",
    "        for unique_text in unique_texts:\n",
    "            unique_text_embedding = semantic_model.encode(unique_text, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(text_embedding, unique_text_embedding).item()\n",
    "            if similarity > threshold:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_texts.append(text)\n",
    "    return unique_texts\n",
    "\n",
    "def video_to_text(video_path, repeat_threshold=5):\n",
    "    \"\"\"Process video frames and generate meaningful text descriptions based on repeated actions.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        st.error(\"Error: Could not open video.\")\n",
    "        return \"\"\n",
    "\n",
    "    current_action = None\n",
    "    action_count = 0\n",
    "    frame_texts = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        text = frame_to_text(frame)\n",
    "\n",
    "        if text == current_action:\n",
    "            action_count += 1\n",
    "        else:\n",
    "            if action_count >= repeat_threshold and current_action is not None:\n",
    "                frame_texts.append(f\"There was '{current_action}'.\")\n",
    "            current_action = text\n",
    "            action_count = 1\n",
    "\n",
    "    if action_count >= repeat_threshold and current_action is not None:\n",
    "        frame_texts.append(f\"The action '{current_action}'.\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    unique_frame_texts = get_unique_meanings(frame_texts)\n",
    "    video_description = \" \".join(unique_frame_texts)\n",
    "    return video_description\n",
    "\n",
    "def translate_text(text, target_lang='ha'):\n",
    "    translation = translator.translate(text, dest=target_lang)\n",
    "    return translation.text\n",
    "\n",
    "def text_to_speech(text, lang='en'):\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(\"output.mp3\")\n",
    "    os.system(\"afplay output.mp3\")\n",
    "\n",
    "# Streamlit app layout\n",
    "st.set_page_config(page_title=\"Video to Text Description\", layout=\"wide\")\n",
    "\n",
    "st.title(\"Video to Text Description and Translation\")\n",
    "st.write(\"Upload a video file to generate a text description, translate it, and convert it to speech.\")\n",
    "\n",
    "# Sidebar for file upload and options\n",
    "st.sidebar.header(\"Upload and Options\")\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload a video file\", type=[\"mp4\", \"mov\", \"avi\", \"mkv\"])\n",
    "target_language = st.sidebar.selectbox(\"Select target language for translation\", ['ha', 'en', 'es', 'fr'])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    with open(\"temp_video.mp4\", \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "\n",
    "    st.video(\"temp_video.mp4\")\n",
    "\n",
    "    if st.sidebar.button(\"Generate Description\"):\n",
    "        with st.spinner(\"Processing...\"):\n",
    "            description = video_to_text(\"temp_video.mp4\")\n",
    "            if description:\n",
    "                st.success(\"Description generated successfully!\")\n",
    "                st.write(\"Original Description:\", description)\n",
    "\n",
    "                translated_description = translate_text(description, target_lang=target_language)\n",
    "                st.write(f\"Translated Description ({target_language}):\", translated_description)\n",
    "\n",
    "                if st.sidebar.button(\"Play Audio\"):\n",
    "                    text_to_speech(translated_description, lang=target_language)\n",
    "                    audio_file = open(\"output.mp3\", \"rb\")\n",
    "                    audio_bytes = audio_file.read()\n",
    "                    st.audio(audio_bytes, format=\"audio/mp3\")\n",
    "else:\n",
    "    st.info(\"Please upload a video file to start.\")\n",
    "\n",
    "# Main content area\n",
    "st.header(\"Instructions\")\n",
    "st.write(\"\"\"\n",
    "1. Upload a video file using the sidebar.\n",
    "2. Select the target language for translation.\n",
    "3. Click on 'Generate Description' to process the video and generate text.\n",
    "4. The translated description will be displayed below the original description.\n",
    "5. Click on 'Play Audio' to listen to the translated description.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0780f9-ff49-454d-b430-9d2f043182f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe275be-8de0-4659-a85c-93a416b4b590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
